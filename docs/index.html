<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers">
  <meta name="keywords" content="Reinforcement Learning, Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers</h1>
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <a href="Taewooknam.notion.site/Taewook-Nam-ae8a9ccb9ca54622b03b3e53541d6241">Taewook Nam</a><sup>1*</sup>
              </span>
              <span class="author-block">
                <a href="https://gimme1dollar.github.io/">Juyong Lee</a><sup>1*</sup>
              </span>
              <span class="author-block">
                <a href="https://jesbu1.github.io/">Jesse Zhang</a><sup>2</sup>
              </span>
                <span class="author-block">
                  <a href="http://www.sungjuhwang.com/">Sung Ju Hwang</a><sup>1</sup> 
                </span>
                <span class="author-block">
                  <a href="https://clvrai.com/web_lim/">Joseph J. Lim</a><sup>1</sup>
                </span>
                <span class="author-block">
                  <a href="https://kpertsch.github.io/">Karl Pertsch</a><sup>3,4</sup>
                </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>KAIST </span>
              <span class="author-block"><sup>2</sup>University of Southern California </span>
              <span class="author-block"><sup>3</sup>University of California, Berkeley </span>
              <span class="author-block"><sup>4</sup>Stanford </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>

            <!-- div class="is-size-5 publication-authors">
              <br>
              <span class="author-block"><b> Conference on Robot Learning 2023</b></span>
            </div-->

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2312.08958" class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/gimme1dollar/lift" class="external-link button is-normal is-rounded is-dark" target="_blank">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    <!--/div-->

  <!-- <br>
<br><hr> -->
  <!--section class="section">
    <div class="container">
      <div class="is-centered has-text-centered">
        <video id="teaser_video" width=100% muted autoplay loop style="border-radius:10px;" margin="auto">
          <source src="static/videos/video.mov">
        </video>
      </div>
    </div>
  </section-->

  <!-- <br><br> -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
          <!--h2 class="title is-3">Abstract</h2-->
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
          <p>
            We propose <b>LiFT</b> (unsupervised <b>L</b>earning w<b>i</b>th <b>F</b>oundation model <b>T</b>eachers) framework that leverages foundation models as teachers, guiding a reinforcement learning agent to acquire semantically meaningful behavior without human feedback. 
          </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <!-- <div class="content has-text-centered">
      <img src="./static/images/boss_overview.png"
            class="interpolation-image"
            alt="Interpolate start reference image.">
      </img>
    </div> -->
    </div>
  </section>
    </div>

  <hr>

  <!-- Animation. -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">LiFT Framework</h2>
      
          <div class="content has-text-justified">
            <p>
              LiFT automates the acquisition of semantically meaningful visuomotor skills with the guidance of foundation models (FMs) for reinforcement learning (RL) agents. 
              Our framework consists of two phases: <b>1) task instruction proposal</b> with the large language model (LLM) and <b>2) multi-task language-conditioned policy learning</b> with the guidance of the vision-language model (VLM).  
            </p>
          </div>
          <!-- Re-rendering. -->
          <!-- <h3 class="title is-4">Method</h3> -->
          <div class="content has-text-centered">
            <img src="./static/images/lift_overview.png" width="600" class="interpolation-image"
              alt="Interpolate start reference image.">
            </img>
          </div>

          <div class="content has-text-justified">
            <h4 class="title is-5">1. LLM Task Instruction Proposal </h4>
            <p>
              LLM is used to generate a grounded set of useful <i>imagined</i> task instructions.
              For example, if the agent is in front of a cow with a bucket in its hand, LLM can propose a task of "milk a cow".
            </p>
            <br>
            <h4 class="title is-5">2. VLM-Guided Policy Learning </h4>
            <p>
              VLM is leveraged to compute the reward measuring the alignment score between the video rollout and corresponding language instructions,
              for training a multi-task policy that performs semantically meaningful behaviors.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <br>
  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- Re-rendering. -->
          <h3 class="title is-3">On Open-ended Environment</h3>
            <p>
              We verify our method in a challenging open-ended MineDojo environment.
              We compare our method with conventional unsupervised RL algorithms with the entropy-maximizing objective.
            </p>
          <br>

          <div class="content has-text-justified">
            <h4 class="title is-5">Success Demonstrations</h4>
            <p> Below images showcase exemplary skill performances by our agent.</p>
            <br>
            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                <div class="column">
                  <img src="./static/images/demo_1.gif" width="250" style='padding-left:10px;' class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <br />
                  <p> "Shear a sheep" </p>
                </div>

                <div class="column">
                  <img src="./static/images/demo_2.gif" width="250" style='padding-left:10px;' class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <p> "Collect logs" </p>
                </div>

                <div class="column">
                  <img src="./static/images/demo_3.gif" width="250" style='padding-left:10px;' class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <p> "Attack a sheep" </p>
                </div>

                <div class="column">
                  <img src="./static/images/demo_4.gif" width="250" style='padding-left:10px;' class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <p> "Combat a zombie" </p>
                </div>

              </div>
            </div>
            <br>

            <h4 class="title is-5">Comparison with Baselines</h4>
            <p>
              Our approach achieves outperforming success rates compared to unsupervised RL baselines (APT and APT w/ MineCLIP) 
              and is even comparable with the baseline using oracle task proposal annotated by humans (LiFT w/ Oracle) in the zero-shot evaluation.
            </p>
            <br>
            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                  <img src="./static/images/comparison_result.png" width="300" class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <br/>
              </div>
            </div>
            <br>

            <p>
              LiFT succeeds in training a policy equipped with semantically meaningful behavior.
              On the other hand, the success rates of unsupervised RL baselines are even less than the VPT baseline.
            </p>
          </div>

          <div class="content has-text-justified">
            <h4 class="title is-5">Behavior Analysis</h4>
          </div>
          
          <div class="content has-text-centered">
            <div class="columns is-centered">
              <!--img src="./static/images/behavior_analysis.png"  width="800" class="interpolation-image" style="width:360" alt="Interpolate start reference image."> </img-->
                <div>
                  <img src="./static/images/analysis_trajectory.png" width="370">
                  </img>
                  <br />
                  <p> (a) Trajectories in top view </p>
                </div>
                <div>
                  <img src="./static/images/analysis_diversity.png" width="470">
                  </img>
                  <br />
                  <p> (b) Normalized reward scores </p>
                </div>
            </div>
          </div>
          <br />
            <p>
              LiFT agents efficiently interact with the target entity, 
              spawned in the green box region in the figure.
              APT agents, however, do not show semantic meaning but 
              just wander around a wide range of regions
              to maxime the objective of diversity.
            </p>
          <br />

        </div>
      </div>
  </section>
  
  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">For Scalable Learning</h2>
            <p>
              We analyze the limitations of current FMs used in our experiment for scalable learning.
              To highlight our design choices for tackling the challenge, we present an ablation experiment result.
            </p>
          <br>

            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                <div>
                  <img src="./static/images/ablation_study.png" width="400" class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <br />
                </div>
              </div>
            </div>

          <div class="content has-text-justified">

            <h4 class="title is-5">Reward Stabilization</h4>
            <p>
              The comparison between the first and second rows in the table shows the effect of reward stabiliziation.
              The below graphs show the difference between the raw VLM reward signals and the processsed version.
            </p>

          <div class="content has-text-centered">
            <div class="columns is-centered">
              <!--img src="./static/images/behavior_analysis.png"  width="800" class="interpolation-image" style="width:360" alt="Interpolate start reference image."> </img-->
                <div>
                  <img src="./static/images/vlm_quality_a.png" width="375">
                  </img>
                  <br />
                  <p> (a) Combat a zombie </p>
                </div>
                <div>
                  <img src="./static/images/vlm_quality_b.png" width="375">
                  </img>
                  <br />
                  <p> (b) Craft planks </p>
                </div>
            </div>
          </div>

            <h4 class="title is-5">Policy Initialization</h4>
            <p>
              The comparison between the first and third rows in the table shows the effect of policy initialization.
              A randomly initialized policy can not learn properly, mainly due to the high complexity of MineDojo environment and the imperfect VLM rewards.
            </p>
            <br>
              
            <h4 class="title is-5">Reward Type</h4>
            <p>
              We additionally experiment on two types of reward definitions: cosine similarity and softmax.
              We conclude that the naive application of softmax reward does not efficiently improve the performance.
            </p>
            <br>

          </div>
        </div>
      </div>
    </div>
  </section>

  <hr>

  <!-- Concurrent Work. -->
  <!-- <section class="section"></section> -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
            nam2023lift,
            title={LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers},
            author={Taewook Nam and Juyong Lee and Jesse Zhang and Sung Ju Hwang and Joseph J Lim and Karl Pertsch},
            year={2023},
            booktitle = {2nd Workshop on Agent Learning in Open-Endedness (ALOE) at NeurIPS 2023},
        }
    </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!--
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>-->
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                Website borrowed from <a rel="license" href="https://nerfies.github.io/">Nerfies</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
  </footer>

</body>

</html>
