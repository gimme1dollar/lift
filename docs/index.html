<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers">
  <meta name="keywords" content="Reinforcement Learning, Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo_clvr.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers</h1>
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <a href="Taewooknam.notion.site/Taewook-Nam-ae8a9ccb9ca54622b03b3e53541d6241">Taewook Nam</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://gimme1dollar.github.io/">Juyong Lee</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://jesbu1.github.io/">Jesse Zhang</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="http://www.sungjuhwang.com/">Sung Ju Hwang</a><sup>1</sup>
                <span class="author-block">
                  <a href="https://clvrai.com/web_lim/">Joseph J. Lim</a><sup>1</sup>
                <span class="author-block">
                  <a href="https://kpertsch.github.io/">Karl Pertsch</a><sup>3</sup>,</span>
                </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>KAIST </span>
              <span class="author-block"><sup>2</sup>University of Southern California </span>
              <span class="author-block"><sup>3</sup>Stanford </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>

            <!-- div class="is-size-5 publication-authors">
              <br>
              <span class="author-block"><b> Conference on Robot Learning 2023</b></span>
            </div-->

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv (Coming Soon)</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    <!--/div-->

  <!-- <br>
<br><hr> -->
  <!--section class="section">
    <div class="container">
      <div class="is-centered has-text-centered">
        <video id="teaser_video" width=100% muted autoplay loop style="border-radius:10px;" margin="auto">
          <source src="static/videos/video.mov">
        </video>
      </div>
    </div>
  </section-->

  <!-- <br><br> -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
          <!--h2 class="title is-3">Abstract</h2-->
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
          <p>
            We propose <b>LiFT</b> (unsupervised <b>L</b>earning w<b>i</b>th <b>F</b>oundation model <b>T</b>eachers) framework that leverages foundation models as teachers, guiding a reinforcement learning agent to acquire semantically meaningful behavior without human intervention. 
          </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <!-- <div class="content has-text-centered">
      <img src="./static/images/boss_overview.png"
            class="interpolation-image"
            alt="Interpolate start reference image.">
      </img>
    </div> -->
    </div>
  </section>
    </div>

  <hr>

  <!-- Animation. -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">LiFT Framework</h2>
      
          <div class="content has-text-justified">
            <p>
              LiFT automates the acquisition of visuomotor skills with the guidance of foundation models (FMs) for reinforcment learning (RL) agents.
              Our framework consists of two phases: <b>1) LLM task instruction proposal</b> which obtains a set of semantically meaningful instruction proposals based on current training environment, 
              and <b>2) VLM-guided policy learning</b> which trains a multi-task policy in the training environment using the obtained instructions.
            </p>
          </div>
          <!-- Re-rendering. -->
          <!-- <h3 class="title is-4">Method</h3> -->
          <div class="content has-text-centered">
            <img src="./static/images/lift_overview.png" width="600" class="interpolation-image"
              alt="Interpolate start reference image.">
            </img>
          </div>

          <div class="content has-text-justified">
            <h4 class="title is-5">1. LLM Task Instruction Proposal </h4>
            <p>
              The first step of our approach is to generate a set of <i>imagined</i> task instructions that are considered to be useful to accomplish future target tasks.
              To achieve this without human supervision, 
              the agent initialized in an environment asks an off-the-shelf LLM to generate useful tasks given the description of the available objects in the current environment.
              The initial state and the proposed text instructions are then used to train a multi-task language-conditioned policy.
            </p>
            <br>
            <h4 class="title is-5">2. VLM-Guided Policy Learning </h4>
            <p>
              Given the proposed set of task instructions and their corresponding initial states, 
              our goal is to train a multi-task policy that follows the instructions.
              To accomplish this without expert demonstrations or human reward engineering,
              we leverage VLMs pre-trained on large, video-language datasets (e.g., YouTube data) 
              to align videos and corresponding language instructions.
              We use these VLMs to directly reward the RL agent based on the task instructions and videos of corresponding agent behavior.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <br>
  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- Re-rendering. -->
          <h3 class="title is-3">On Open-ended Environment</h3>
            <p>
              We verify our method in a challenging open-ended MineDojo environment.
              We train RL agents in training environments and, then, test on a set of new environments with target task descriptions.
              We compare our method with conventional unsupervised RL algorithms with entropy-maximizing objective.
            </p>
          <br>

          <div class="content has-text-justified">
            <h4 class="title is-5">Comparison with Baselines</h4>
            <p>
              Our approach achieves outperforming success rates compared to unsupervised RL baselines (APT and APT w/ MineCLIP) 
              and even comparable with the baseline using human supervision for task proposal (LiFT w/o LLM) with zero-shot evaluation.
            </p>
            <br>
            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                  <img src="./static/images/comparison_result.png" width="300" class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <br/>
              </div>
            </div>

            <p>
              LiFT succeeds to train a policy equipped with semantically meaningful behavior.
              (e.g., learning how to shear a sheep with initialization position near a sheep with a shear.)
              On the other hand, the success rates of unsupervised RL baselines are even less than the VPT baseline.
            </p>
          </div>

          <div class="content has-text-justified">
            <h4 class="title is-5">Behavior Analysis</h4>
          </div>
          <div class="columns is-centered">
            <img src="./static/images/behavior_analysis.png"  width="800" class="interpolation-image" style="width:360"
              alt="Interpolate start reference image.">
            </img>
          </div>
          <br />
            <p>
              The trajectories from LiFT agent exhibit efficient approaching behaviors toward the target, 
              compared to the undirected random trajectories from APT agents.
              While the APT agents try maximizing the diversity as shown with the highest scores on each reward metric, 
              the learned behavior with conventional unsupervised RL algorithm is yet semantically meaningful from that 
              the agents just wander around a wide range of regions or gaze on diverse entities without following the task description.
            </p>
          <br />

        </div>
      </div>
  </section>
  
  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Toward Scalable Learning</h2>
            <p>
              We examine the effects of each strategy used in our experiment.
              We, then, analyze limitations of current FMs used in our experiment for scaling learning with suggesting possible future directions.
            </p>
          <br>
          <div class="content has-text-justified">

            <h4 class="title is-5">Ablation Study on the Strategies</h4>
            <p>
              To highlight our design choices on tackling the challenge as well as future directions, 
              we present an ablation experiment result.
            </p>
            <br>

            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                  <img src="./static/images/ablation_study.png" width="300" class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
              </div>
            </div>

            <ol>
              <br>
              <li><b>Reward Stabilization.</b>
                <br>
                We empirically observe that the computed reward signal typically exhibits high variance, 
                likely coming from noise induced by visual variations within the observed agent trajectories.
                Thus, we adapt the reward stabilization technique of both smoothing and clipping to mitigate learning instability.
                </br>
              </li>
              <br>
              <li><b>Policy Initialization.</b>
                <br>
                A randomly initialized policy is not able to learn meaningful behavior 
                due to the complexity of MineDojo environment and the imperfect reward quality from VLM.
                This learning failure primarily stems from the challenge of attempting to learn long-horizon and high-dimensional Minecraft skills entirely from scratch.
                </br>
              </li>
              <br>
              <li><b>Reward Definition.</b>
                <br>
                We explore definitions for the reward function, 
                aiming at assessing the potential of relative normalization through softmax rewards for reward stabilization.
                Yet, we conclude that the naive application of softmax reward does not efficiently improve the evaluation result,
                as we obtain a higher performance score with reward defined with cosine similarity.
               </br>
              </li>
            </ol>

            <br>

            <h4 class="title is-5">Diagnosis on VLM Reward Quality</h4>
            
            <br>
            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                  <img src="./static/images/vlm_quality.png" width="800" class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <br/>
              </div>
            </div>
            <br>

            <p>
              VLM computed rewards in the figure above illustrate that 
              VLM rewards are generally high for behavior that is somewhat related to the task instructions 
              but often fall short of providing the highest rewards for the most success-critical behavior.
              We suspect that this imperfection arises from the video-language alignment issue with the current off-the-shelf VLM.
            </p>

            <p>
              This work has attempted to alleviate the issues by using several strategies discussed above, 
              yet we leave improving VLM reward for a more diverse set of tasks, including complex and delicate tasks, as future work with possible directions below:

              <li><b>Internet-scale Dataset</b>
                may have been a cause for misalignment.
                Many of Youtube video data used for training MineCLIP have issues of misalignment with the caption of the vocal used as the text label.
                We anticipate that this challenge can be mitigated through fine-tuning with the utilization of in-domain expert data with better alginment quality.
              </li>
              <br>
              <li><b>Contrastive Objective</b>
                without considering of temporal precision might have not been appropriate for fine-grained feedback on motion.
                The <a href="">similar observation</a> has been reported with video-language alignment score of R3M on robot manipulation tasks. 
                We believe that a more appealing design of architectural design or objective  more suitable for the fine-grained temporal understanding for video-language alignment is demanding. 
              </li>
            </p>

            <br>
          </div>
        </div>
      </div>
    </div>
  </section>

  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Q&A<br>
          </h2>
          <div class="content has-text-justified">

            <h4 class="title is-5">What does LiFT present on top of
              <a href="https://arxiv.org/abs/2206.08853">MineCLIP</a>?</h4>
            <p>
              MineDojo proposes set of new tasks brainstormed with LLM and to reward agent using VLM trained on the YouTube knowledge base, 
              while how to integrate and ground them on an open-ended learning environment is yet explored.
              We focus on closing the loop for adaptation to open-ended environments.
            </p>
            <br>

            <h4 class="title is-5">How is LiFT different from
              <a href="https://arxiv.org/abs/2302.06692">ELLM</a>?</h4>
            <p>
              ELLM proposes a method of using LLM for guiding the pretraining agents for exploration.
              We note that the LiFT agents have access to a text description of the interactable objects available in the environment,
              and this assumption is much simpler yet more practical.
              Similar to ELLM, a line of work specifically focus on using FMs for Minecraft domain, while none of works integrate pre-trained LLM and VLM for an unsupervised RL system.
            </p>
            <br>

            <h4 class="title is-5">How can LiFT be complementary to
              <a href="https://arxiv.org/abs/2305.16291">Voyager</a>?</h4>
            <p>
              Voyager introduces a lifelong learning framework that continually explore the world and extend skill set in MineDojo environment using LLM.
              Voyager uses LLM for code-level planning and refinement based on pre-defined basic skills, 
              while our work aims to acquire visuo-motor policy without access to hand-engineered skills.
            </p>
            <br>

          </div>
        </div>
      </div>
    </div>
  </section>

  <hr>

  <!-- Concurrent Work. -->
  <!-- <section class="section"></section> -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
    nam2023lift,
    title={LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers},
    author={Taewook Nam and Juyong Lee and Jesse Zhang and Sung Ju Hwang and Joseph J Lim and Karl Pertsch},
    year={2023},
    url={https://github.com/gimme1dollar/lift},
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!--
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>-->
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                Website borrowed from <a rel="license" href="https://nerfies.github.io/">Nerfies</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
  </footer>

</body>

</html>
