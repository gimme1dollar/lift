<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers">
  <meta name="keywords" content="Reinforcement Learning, Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers</h1>
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <a href="Taewooknam.notion.site/Taewook-Nam-ae8a9ccb9ca54622b03b3e53541d6241">Taewook Nam</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://gimme1dollar.github.io/">Juyong Lee</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://jesbu1.github.io/">Jesse Zhang</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="http://www.sungjuhwang.com/">Sung Ju Hwang</a><sup>1</sup>
                <span class="author-block">
                  <a href="https://clvrai.com/web_lim/">Joseph J. Lim</a><sup>1</sup>
                <span class="author-block">
                  <a href="https://kpertsch.github.io/">Karl Pertsch</a><sup>3</sup>,</span>
                </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>KAIST </span>
              <span class="author-block"><sup>2</sup>University of Southern California </span>
              <span class="author-block"><sup>3</sup>Stanford </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div>

            <!-- div class="is-size-5 publication-authors">
              <br>
              <span class="author-block"><b> Conference on Robot Learning 2023</b></span>
            </div-->

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv (Coming Soon)</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    <!--/div-->

  <!-- <br>
<br><hr> -->
  <!--section class="section">
    <div class="container">
      <div class="is-centered has-text-centered">
        <video id="teaser_video" width=100% muted autoplay loop style="border-radius:10px;" margin="auto">
          <source src="static/videos/video.mov">
        </video>
      </div>
    </div>
  </section-->

  <!-- <br><br> -->
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
          <!--h2 class="title is-3">Abstract</h2-->
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
          <p>
            We propose <b>LiFT</b> (unsupervised <b>L</b>earning w<b>i</b>th <b>F</b>oundation model <b>T</b>eachers) framework that leverages foundation models as teachers, guiding a reinforcement learning agent to acquire semantically meaningful behavior without human intervention. 
          </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <!-- <div class="content has-text-centered">
      <img src="./static/images/boss_overview.png"
            class="interpolation-image"
            alt="Interpolate start reference image.">
      </img>
    </div> -->
    </div>
  </section>
    </div>

  <hr>

  <!-- Animation. -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">LiFT Framework</h2>
      
          <div class="content has-text-justified">
            <p>
              LiFT automates the acquisition of semantically meaningful visuomotor skills with the guidance of foundation models (FMs) for reinforcement learning (RL) agents. 
              Our framework consists of two phases: <b>1) task instruction proposal</b> with the large language model (LLM) and <b>2) multi-task language-conditioned policy learning</b> with the guidance of the vision-language model (VLM).  
            </p>
          </div>
          <!-- Re-rendering. -->
          <!-- <h3 class="title is-4">Method</h3> -->
          <div class="content has-text-centered">
            <img src="./static/images/lift_overview.png" width="600" class="interpolation-image"
              alt="Interpolate start reference image.">
            </img>
          </div>

          <div class="content has-text-justified">
            <h4 class="title is-5">1. LLM Task Instruction Proposal </h4>
            <p>
              LLM is used to generate a grounded set of useful <i>imagined</i> task instructions.
              For example, if the agent is in front of a cow with a bucket in its hand, LLM proposes "to milk a cow".
            </p>
            <br>
            <h4 class="title is-5">2. VLM-Guided Policy Learning </h4>
            <p>
              VLM is leveraged to compute the reward measuring the alignment score between the video rollout and corresponding language instructions,
              for training a multi-task policy that performs semantically meaningful behaviors.
            </p>

          </div>
        </div>
      </div>
    </div>
  </section>

  <br>
  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- Re-rendering. -->
          <h3 class="title is-3">On Open-ended Environment</h3>
            <p>
              We verify our method in a challenging open-ended MineDojo environment.
              We compare our method with conventional unsupervised RL algorithms with the entropy-maximizing objective.
            </p>
          <br>

          <div class="content has-text-justified">
            <h4 class="title is-5">Success Demonstrations</h4>
            <p> Below images showcase exemplary skill performances by our agents.</p>
            <br>
            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                <div class="column">
                  <img src="./static/images/demo_1.gif" width="250" style='padding-left:10px;' class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <br />
                  <p> "Shear a sheep" </p>
                </div>

                <div class="column">
                  <img src="./static/images/demo_2.gif" width="250" style='padding-left:10px;' class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <p> "Collect logs" </p>
                </div>

                <div class="column">
                  <img src="./static/images/demo_3.gif" width="250" style='padding-left:10px;' class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <p> "Attack a sheep" </p>
                </div>

                <div class="column">
                  <img src="./static/images/demo_4.gif" width="250" style='padding-left:10px;' class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <p> "Combat a zombie" </p>
                </div>

              </div>
            </div>
            <br>

            <h4 class="title is-5">Comparison with Baselines</h4>
            <p>
              Our approach achieves outperforming success rates compared to unsupervised RL baselines (APT and APT w/ MineCLIP) 
              and is even comparable with the baseline using human supervision for task proposal (LiFT w/o LLM) in the zero-shot evaluation.
            </p>
            <br>
            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                  <img src="./static/images/comparison_result.png" width="300" class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <br/>
              </div>
            </div>
            <br>

            <p>
              LiFT succeeds in training a policy equipped with semantically meaningful behavior.
              On the other hand, the success rates of unsupervised RL baselines are even less than the VPT baseline.
            </p>
          </div>

          <div class="content has-text-justified">
            <h4 class="title is-5">Behavior Analysis</h4>
          </div>
          <div class="columns is-centered">
            <img src="./static/images/behavior_analysis.png"  width="800" class="interpolation-image" style="width:360"
              alt="Interpolate start reference image.">
            </img>
          </div>
          <br />
            <p>
              <!--
              The trajectories from LiFT agent exhibit efficient approaching behaviors toward the target, 
              compared to the undirected random trajectories from APT agents.
              -->
              LiFT agents exhibit efficient trajectories toward the target entity.
              APT agents, however, do not show semantic meaning but 
              just wander around a wide range of regions or gaze on diverse entities,
              despite trying maximizing the objective of diversity.
            </p>
          <br />

        </div>
      </div>
  </section>
  
  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Toward Scalable Learning</h2>
            <p>
              We examine the effects of each strategy used in our experiment.
              We, then, analyze the limitations of current FMs used in our experiment for scaling learning and suggest possible future directions.
            </p>
          <br>
          <div class="content has-text-justified">

            <h4 class="title is-5">Ablation Study on the Strategies</h4>
            <p>
              To highlight our design choices for tackling the challenge, 
              we present an ablation experiment result.
            </p>
            <br>

            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                  <img src="./static/images/ablation_study.png" width="300" class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
              </div>
            </div>

            <ol>
              <li><b>Reward Definition.</b>
                <br>
                We conclude that the naive application of softmax reward does not efficiently improve the performance.
               </br>
              <br>
              </li>
              <li><b>Reward Stabilization.</b>
                <br>
                We empirically observe that the computed reward signal typically exhibits high variance, 
                likely coming from noise induced by visual variations within the observed agent trajectories.
                </br>
              </li>
              <br>
              <li><b>Policy Initialization.</b>
                <br>
                A randomly initialized policy is not able to learn meaningful behavior 
                due to the high complexity of MineDojo environment and the imperfect reward quality from VLM.
                </br>
              </li>
              <br>
            </ol>

            <br>

            <h4 class="title is-5">Diagnosis on VLM Reward Quality</h4>
            
            <br>
            <div class="content has-text-centered">
              <!--  2 gif images place in one row -->
              <div class="columns is-centered">
                  <img src="./static/images/vlm_quality.png" width="800" class="interpolation-image"
                    alt="Interpolate start reference image.">
                  </img>
                  <br/>
              </div>
            </div>
            <br>

            <p>
              VLM rewards are generally high for behavior that is somewhat related to the task instructions 
              but often fall short of providing the highest rewards for the most success-critical behavior.
            </p>

            <p>
              We leave improving VLM reward for future work with possible directions below:

              <li><b>Internet-scale Dataset</b>
                may have been a cause for misalignment.
                We anticipate that this challenge can be mitigated through fine-tuning with the utilization of in-domain expert data with better alignment quality.
              </li>
              <br>
              <li><b>Contrastive Objective</b>
                without considering temporal precision might have not been appropriate for fine-grained feedback on the motion.
                We believe that a more appealing design of architectural design or objective is demanding. 
              </li>
            </p>

            <br>
          </div>
        </div>
      </div>
    </div>
  </section>

  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Q&A<br>
          </h2>
          <div class="content has-text-justified">

            <h4 class="title is-5">What does LiFT present on top of
              <a href="https://arxiv.org/abs/2206.08853">MineCLIP</a>?</h4>
            <p>
              MineDojo proposes a set of new tasks brainstormed with LLM and to reward agents using VLM trained on the YouTube knowledge base, 
              while how to integrate and ground them on an open-ended learning environment is yet explored.
              We focus on closing the loop for adaptation to open-ended environments.
            </p>
            <br>

            <h4 class="title is-5">How is LiFT different from
              <a href="https://arxiv.org/abs/2302.06692">ELLM</a>?</h4>
            <p>
              ELLM proposes a method of using LLM for guiding the pretraining agents for exploration.
              We note that the LiFT agents have access to a text description of the interactable objects available in the environment,
              and this assumption is much simpler yet more practical.
              Similar to ELLM, a line of work specifically focuses on using FMs for Minecraft domain, while none of the works integrate pre-trained LLM and VLM for an unsupervised RL system.
            </p>
            <br>

            <h4 class="title is-5">How can LiFT be complementary to
              <a href="https://arxiv.org/abs/2305.16291">Voyager</a>?</h4>
            <p>
              Voyager introduces a lifelong learning framework that continually explores the world and extends the skill set in MineDojo environment using LLM.
              Voyager uses LLM for code-level planning and refinement based on pre-defined basic skills, 
              while our work aims to acquire visuo-motor policy without access to hand-engineered skills.
            </p>
            <br>

          </div>
        </div>
      </div>
    </div>
  </section>

  <hr>

  <!-- Concurrent Work. -->
  <!-- <section class="section"></section> -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
    nam2023lift,
    title={LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers},
    author={Taewook Nam and Juyong Lee and Jesse Zhang and Sung Ju Hwang and Joseph J Lim and Karl Pertsch},
    year={2023},
    url={https://github.com/gimme1dollar/lift},
}</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <!--
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>-->
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                Website borrowed from <a rel="license" href="https://nerfies.github.io/">Nerfies</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
  </footer>

</body>

</html>
